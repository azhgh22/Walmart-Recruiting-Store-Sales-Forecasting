# შესავალი

მოცემული პროექტი ეფუძნება **Walmart Recruiting - Store Sales Forecasting**, რომლის მიზანია Walmart-ის მაღაზიების გაყიდვების ზუსტი პროგნოზირება.

პროექტზე მუშაობისას ჩვენ გამოვიკვლიეთ და გავტესტეთ დროითი მწკრივების პროგნოზირებისთვის არსებული მრავალი ცნობილი მეთოდი. მათ შორის:

*   **კლასიკური სტატისტიკური მოდელები:** ARIMA, SARIMA, SARIMAX.
*   **ხეზე დაფუძნებული მოდელები:** XGBoost, LightGBM.
*   **Deep Learning მოდელები დროითი მწკრივებისთვის:** PathTST, N-BEATS, TFT, DLinear.

არსებული მოდელების ტესტირების პარალელურად, ჩვენ შევიმუშავეთ რამდენიმე მოდელის გაერთიანების (ensembling) იდეები და შევქმენით ჩვენი საკუთარი, `group_stat` მოდელები, რომელთა მთავარი პრინციპიც რამდენიმე განსხვავებული მიდგომის გაერთიანებაა. ასევე, შეიქმნა მათი რამდენიმე გაუმჯობესებული ვერსია.

თითოეული მიდგომისა და მოდელის შესახებ დეტალურად მომდევნო სექციებში ვისაუბრებთ.

# EDA (მონაცემთა კვლევითი ანალიზი)

მონაცემთა კვლევითი ანალიზის (EDA) პროცესი დეტალურად არის წარმოდგენილი `notebooks/01_eda.ipynb` ფაილში. ანალიზის შედეგად რამდენიმე მნიშვნელოვანი პატერნი და თვისება გამოვლინდა:

*   **ტრენდი და სეზონურობა:** მონაცემებს არ გააჩნია გამოკვეთილი გრძელვადიანი ტრენდი (მაგალითად, გაყიდვების ზრდა ან კლება წლიდან წლამდე). თუმცა, ნათლად ჩანს ძლიერი **სეზონურობა** — გაყიდვების გრაფიკი ყოველწლიურად თითქმის იდენტურ ფორმას იმეორებს.

*   **დეპარტამენტების მსგავსება მაღაზიებს შორის:** აღმოჩნდა, რომ კონკრეტული დეპარტამენტის გაყიდვების გრაფიკს სხვადასხვა მაღაზიაში ძალიან მსგავსი ფორმა აქვს. მაგალითად, სათამაშოების დეპარტამენტის გაყიდვები ყველა მაღაზიაში ზამთარში აღწევს პიკს. მთავარი განსხვავება მხოლოდ გაყიდვების მოცულობაშია, რაც, სავარაუდოდ, მაღაზიის ზომით ან რეგიონის მოსახლეობის რაოდენობით არის განპირობებული. ფაქტობრივად, ფუნქციის სახე იდენტურია და მხოლოდ კონსტანტით (`offset`) არის გადაადგილებული.

*   **მაღაზიების საშუალო გაყიდვების მსგავსება:** მსგავსი პატერნი შეინიშნება ცალკეული მაღაზიების საშუალო შემოსავლიანობაშიც. თითქმის ყველა მაღაზიის გაყიდვები პიკს აღწევს საახალწლო პერიოდში, ხოლო სხვა დროს მსგავს დონეზე ნარჩუნდება.

*   **ავტოკორელაცია და Lag-მახასიათებლები:** ავტოკორელაციის ფუნქციის ანალიზმა აჩვენა, რომ მიმდინარე კვირის გაყიდვები მნიშვნელოვნად არის დამოკიდებული წინა 2, 3 და 4 კვირის მონაცემებზე (მოკლევადიანი დამოკიდებულება), ასევე წინა წლის იმავე კვირის გაყიდვებზე (გრძელვადიანი, სეზონური დამოკიდებულება). ეს გვაძლევს იმის საფუძველს, რომ მოდელისთვის შევქმნათ როგორც მოკლევადიანი (`lag`), ასევე გრძელვადიანი მახასიათებლები. მაგალითად, მნიშვნელოვანი მახასიათებელი შეიძლება იყოს წლის კონკრეტულ კვირაში (ვთქვათ, მე-5 კვირას) გაყიდვების საშუალო მაჩვენებელი წინა წლების განმავლობაში.

ამ დაკვირვებებმა გვიბიძგა მთავარი იდეისკენ: იმის მაგივრად, რომ მხოლოდ ისტორიულ საშუალო მაჩვენებლებს დავყრდნობოდით, ჩვენ შეგვიძლია შევქმნათ უფრო დინამიური მახასიათებლები. კერძოდ, ყოველი მონაცემისთვის დავამატოთ ორი ახალი მახასიათებელი (feature):

1.  **`dept_predicted_sales`**: პროგნოზი, რომელიც გაკეთებულია მხოლოდ დეპარტამენტის დონეზე (ყველა მაღაზიის მონაცემის აგრეგირებით).
2.  **`store_predicted_sales`**: პროგნოზი, რომელიც გაკეთებულია მხოლოდ მაღაზიის დონეზე (ყველა დეპარტამენტის მონაცემის აგრეგირებით).

ამ მახასიათებლების მისაღებად, ჩვენ ჯერ ცალკეული მოდელებით ვასწავლით და ვაპროგნოზირებთ `store_avg` და `dept_avg` გაყიდვებს, შემდეგ კი ამ პროგნოზებს, როგორც მძლავრ მახასიათებლებს, ვუმატებთ ჩვენს მთავარ მოდელს. სწორედ ეს მიდგომა უდევს საფუძვლად ჩვენს `group_stat` მოდელს.

# მოდელების მიმოხილვა

ამ სექციაში განვიხილავთ ყველა იმ მოდელს, რომელიც პროექტის ფარგლებში გავტესტეთ, დაწყებული კლასიკური სტატისტიკური მეთოდებიდან, დასრულებული თანამედროვე Deep Learning არქიტექტურებით.

## სტატისტიკური მოდელები

ქვემოთ მოცემული სამივე მოდელისთვის გამოვიყენეთ მახასიათებლების ინჟინერიის (feature engineering) შემდეგი მეთოდები:

*   **არაინფორმაციული მახასიათებლების წაშლა:** `MarkDown` სვეტები ნაკლებად სასარგებლო ინფორმაციას შეიცავდა, ამიტომ ისინი წავშალეთ.
*   **კატეგორიული ცვლადების კოდირება:** ვინაიდან წრფივი მოდელები მხოლოდ რიცხვით მონაცემებზე მუშაობს, კატეგორიული მახასიათებლებისთვის (`IsHoliday`, `Type`) გამოვიყენეთ `one-hot encoding` მეთოდი.

### ARIMA
ARIMA დროითი მწკრივების ანალიზის ერთ-ერთი ყველაზე მარტივი და კლასიკური სტატისტიკური მოდელია. მისი შედეგები ვალიდაციის მონაცემებზე იყო **4195.88**, ხოლო სატრენინგო მონაცემებზე — **2660.10**.

ეს საკმაოდ სუსტი შედეგია, რაც მოსალოდნელიც იყო, რადგან მოდელს რამდენიმე მნიშვნელოვანი ნაკლი აქვს:
*   **წრფივობა:** ის ეფუძნება წრფივ დაშვებებს, ჩვენი მონაცემების სტრუქტურა კი აშკარად არაწრფივია.
*   **გარე მახასიათებლების იგნორირება:** მოდელი არ ითვალისწინებს დამატებით მახასიათებლებს(`features`), როგორიცაა სადღესასწაულო დღეები, ტემპერატურა და ა.შ.

### SARIMA
SARIMA წარმოადგენს ARIMA მოდელის გაუმჯობესებულ ვერსიას, რომელიც დამატებით ითვალისწინებს **სეზონურობას**. რადგან ჩვენს მონაცემებს წლიური სეზონურობა აქვთ, ამ მოდელმა უკეთ უნდა იმუშავოს.

მოდელის დასწავლის დრო (`runtime`) ძალიან დიდი აღმოჩნდა, ამიტომ გადავწყვიტეთ, ექსპერიმენტი ჩაგვეტარებინა მონაცემთა ნაწილზე (`sampling`) — შემთხვევითად შევარჩიეთ რამდენიმე `store/dept` წყვილი. შედეგები დასემპლილ მონაცემებზე ასეთი იყო: **Train WMAE: 21910.49**, **Valid WMAE: 22466.55**.

მიუხედავად იმისა, რომ მოდელი თეორიულად უფრო მძლავრია, სრულ მონაცემთა ბაზაზე მისი გაშვება და პროგნოზის გაკეთება გამოთვლითი სირთულის გამო პრაქტიკულად შეუძლებელია.

### SARIMAX
იგივე დასემპლილ მონაცემებზე გავტესტეთ SARIMAX მოდელიც, რომელიც SARIMA-ს საშუალებას აძლევს, გაითვალისწინოს გარე მახასიათებლები (`eXogenous features`). შედეგები ასეთი იყო: **Train WMAE: 578148.08**, **Valid WMAE: 308131.33**.

როგორც ვხედავთ, შედეგი მნიშვნელოვნად გაუარესდა SARIMA-სთან შედარებითაც კი. ეს ნიშნავს, რომ დამატებითი მახასიათებლების ჩართვამ მოდელის ხარისხი შეამცირა. ამის სავარაუდო მიზეზებია:

*   SARIMAX მოდელი შეიძლება ზედმეტად ხისტი იყოს მონაცემებში არსებული რთული ურთიერთდამოკიდებულებების აღსაქმელად.
*   დამატებითმა მახასიათებლებმა შესაძლოა „ხმაური“ შეიტანა მოდელში, თუ ისინი არასწორად იქნა გამოყენებული.
*   შერჩეული მონაცემების სიმცირემ (სემპლინგის გამო) შესაძლოა გააძლიერა გადაჭარბებული მორგების (overfitting) ეფექტი, როდესაც მოდელს მეტი ცვლადი დაემატა.

შედეგები შეგიძლიათ ნახოთ: `notebooks/02_linear_models.ipynb`.

## ხეზე დაფუძნებული მოდელები

### XGBoost

პროექტში გავტესტეთ XGBoost, რომელიც ერთ-ერთი ყველაზე მძლავრი მოდელია. აღსანიშნავია, რომ LightGBM-ს მსგავსი პრინციპი აქვს, თუმცა ის უფრო ეფექტურია დიდ მონაცემთა ბაზებზე. ამ ეტაპზე ფოკუსირება XGBoost-ზე გავაკეთეთ.

XGBoost-ის მთავარი ნაკლი ისაა, რომ ის ბუნებრივად არ ითვალისწინებს დროითი მწკრივის სტრუქტურას, ანუ წარსული მოვლენების პირდაპირ დამოკიდებულებას მომავალზე. თუმცა, ამის კომპენსირება შესაძლებელია სწორი მახასიათებლების ინჟინერიით (`feature engineering`).

ჩვენ დავამატეთ შემდეგი მახასიათებლები:
*   წელიწადის კვირის ნომერი
*   სადღესასწაულო დღის ინდიკატორი
*   დრო, რომელიც დარჩა მომდევნო დღესასწაულამდე
*   ფურიეს გარდაქმნის მახასიათებლები (სეზონურობის აღსაქმელად)

ამ მახასიათებლებით მივიღეთ შემდეგი შედეგი:
-   **Train WMAE:** 1632.75
-   **Valid WMAE:** 2893.66

ეს შედეგი მნიშვნელოვნად აღემატება სტატისტიკური მოდელებისას.

### XGBoost ავტორეგრესიული მიდგომით

შემდეგ ვცადეთ XGBoost-ის ავტორეგრესიული მოდელირება. ამ მიდგომისას, მახასიათებლებად დავამატეთ `lag`-ები: წინა 2, 3 და 51-ე კვირის გაყიდვების მონაცემები. პროგნოზირების პროცესში, ეს `lag`-ები დინამიურად ივსებოდა ჩვენივე წინა პროგნოზებით.

შედეგები ასეთი იყო:
-   **Train WMAE:** 3001.25
-   **Valid WMAE:** 3232.62

როგორც ვხედავთ, შედეგი **გაუარესდა**. ეს იმას მიუთითებს, რომ მოდელის მიერ ერთხელ დაშვებული შეცდომა (არასწორი პროგნოზი) შემდეგ ნაბიჯზე გადადის და `lag`-ის სახით ახალ შეცდომას იწვევს, რაც საბოლოოდ აკუმულირდება და აუარესებს საერთო ხარისხს. განსაკუთრებით პრობლემურია ის `store/dept` წყვილები, რომლებზეც ცოტა მონაცემი გვაქვს, რადგან მათზე არაზუსტი პროგნოზი გარდაუვალია და ამ შეცდომაზე დაყრდნობა მოდელს არასწორი მიმართულებით წაიყვანს.

## ნეირონული ქსელები (Deep Learning Models)

პროექტის ფარგლებში, ასევე გამოვცადეთ დროითი მწკრივებისთვის განკუთვნილი ოთხი ნეირონული ქსელის არქიტექტურა. თითოეული მოდელისთვის ჩატარდა ჰიპერპარამეტრების ოპტიმიზაცია (tuning), რის შედეგადაც მივიღეთ საინტერესო შედეგები.

ქვემოთ მოცემულია გამოყენებული მოდელების ძირითადი ჰიპერპარამეტრები:

| მოდელი | მნიშვნელოვანი ჰიპერპარამეტრები |
| :--- | :--- |
| **N-BEATS** | `input_size=52`, `h=53`, `learning_rate=1e-3`, `batch_size=256`, `optimizer=AdamW`, `shared_weights=True` |
| **D-Linear** | `input_size=60`, `h=53`, `learning_rate=1e-2`, `batch_size=512`, `optimizer=Adagrad`, `scaler_type='robust'` |
| **PatchTST** | `input_size=52`, `h=53`, `dropout=0.2`, `batch_size=64`, `activation='relu'` |
| **TFT** | `input_size=60`, `h=53`, `dropout=0.1`, `max_steps=20*104` |

*შენიშვნა: `h=53` ნიშნავს, რომ მოდელი აპროგნოზირებს მომდევნო 53 კვირის მონაცემებს.*

### შედეგები (ვალიდაციის WMAE)

აღსანიშნავია, რომ გამოყენებული `neuralforecast` ბიბლიოთეკა არ ითვლის სატრენინგო (train) მონაცემებზე შეცდომას, ამიტომ წარმოდგენილია მხოლოდ ვალიდაციის შედეგები.

| მოდელი | ვალიდაციის WMAE |
| :--- | :--- |
| **PatchTST** | **1526.46** |
| **N-BEATS** | 1587.59 |
| **D-Linear** | 1598.11 |
| **TFT** | 1717.15 |

### ანალიზი

როგორც შედეგებიდან ჩანს, საუკეთესო ხარისხი აჩვენა **PatchTST** მოდელმა, რომელმაც ყველა წინა მოდელზე (სტატისტიკურსა და ხეზე დაფუძნებულზე) მნიშვნელოვნად უკეთესი შედეგი დადო. მას მცირედით ჩამორჩებიან N-BEATS და D-Linear, ხოლო TFT-ის შედეგი შედარებით სუსტია.

ყველაზე საინტერესო და, გარკვეულწილად, მოულოდნელი დაკვირვება ისაა, რომ ეს შედეგები მიღწეულია **ყოველგვარი გარე მახასიათებლების (features) გარეშე**. ნეირონულმა ქსელებმა შეძლეს მხოლოდ დროითი მწკრივის ისტორიულ მონაცემებზე დაყრდნობით (univariate approach) აღექვათ რთული სეზონური პატერნები და დამოკიდებულებები. ეს მიუთითებს იმაზე, რომ ამ მონაცემებში საკმარისი ინფორმაციაა მაღალი სიზუსტის პროგნოზის მისაღებად და, შესაძლოა, მხოლოდ წინა გაყიდვების დინამიკის ანალიზითაც შეიძლება კარგი შედეგის მიღწევა.

# ნეირონული ქსელების ანსამბლი (Ensemble)

ნეირონული ქსელების ინდივიდუალური შედეგების გაანალიზებისას საინტერესო ტენდენცია გამოიკვეთა. როდესაც პროგნოზების გრაფიკებს ვადარებდით რეალურ გაყიდვებს (`Weekly_Sales`), შევნიშნეთ შემდეგი:

*   **N-BEATS** მოდელი მუდმივად აკეთებდა "ფრთხილ" პროგნოზს და სისტემატურად ამცირებდა (underestimate) რეალურ გაყიდვებს.
*   **D-Linear** მოდელი, პირიქით, ავლენდა საპირისპირო ქცევას და სისტემატურად ზრდიდა (overestimate) რეალურ გაყიდვებს.
*   სხვა მოდელები, როგორიცაა **PatchTST**, უფრო არასტაბილური იყო — ზოგჯერ ამცირებდა, ზოგჯერ კი ზრდიდა პროგნოზს.

ამ დაკვირვებამ გვიბიძგა ანსამბლის მოდელის (Ensemble) შექმნისკენ. ლოგიკა მარტივია: თუ ერთ მოდელს მიდრეკილება აქვს შეამციროს შედეგი, ხოლო მეორეს — გაზარდოს, მათი პროგნოზების გასაშუალოება, სავარაუდოდ, ერთმანეთს დააბალანსებს და უფრო მიახლოებულ შედეგს მოგვცემს.

შედეგად, შევქმენით **`nn_ensemble`** მოდელი, რომელიც წარმოადგენს ოთხივე ნეირონული ქსელის (PatchTST, N-BEATS, D-Linear, TFT) პროგნოზების საშუალო არითმეტიკულს.

ვალიდაციის შედეგმა ჩვენი მოლოდინი სრულად გაამართლა:

-   **`nn_ensemble` ვალიდაციის WMAE: 1467.43**

ეს შედეგი მნიშვნელოვნად უკეთესია, ვიდრე ნებისმიერი ცალკეული ნეირონული ქსელის შედეგი. ანსამბლის შექმნით ჩვენ შევძელით ცალკეული მოდელების სისტემური შეცდომების განეიტრალება და მივიღეთ უფრო სტაბილური და ზუსტი პროგნოზი. ეს კიდევ ერთხელ ადასტურებს, რომ განსხვავებული მიდგომების მქონე მოდელების გაერთიანება ხშირად საუკეთესო სტრატეგიაა.

# GroupStat: ჯგუფების სტატისტიკაზე დაფუძნებული მოდელი

როგორც EDA სექციაში აღვნიშნეთ, ჩვენი მთავარი ჰიპოთეზა იყო, რომ მახასიათებლები `Store` და `Dept` თავისთავად (როგორც რიცხვითი იდენტიფიკატორები) ნაკლებად ინფორმაციულია. ბევრად უფრო მნიშვნელოვანია ვიცოდეთ:
1.  რა იყო კონკრეტული **მაღაზიის** საშუალო გაყიდვები იმავე კვირაში?
2.  რა იყო კონკრეტული **დეპარტამენტის** საშუალო გაყიდვები სხვა მაღაზიებში იმავე კვირაში?

რადგან საპროგნოზო პერიოდისთვის ეს მონაცემები უცნობია, ჩვენ გადავწყვიტეთ, ისინიც ცალკე მოდელებით გვეპროგნოზირებინა. ამრიგად, GroupStat მოდელის სტრუქტურა სამსაფეხურიანია:
1.  მაღაზიების საშუალო გაყიდვების პროგნოზირება.
2.  დეპარტამენტების საშუალო გაყიდვების პროგნოზირება.
3.  ამ ორი პროგნოზის, როგორც ახალი მახასიათებლების, გამოყენება საბოლოო მოდელში.

## მახასიათებლების ინჟინერია (Feature Engineering)

ყველა ეტაპზე გამოყენებული მოდელებისთვის, საწყის მონაცემებს დავამატეთ შემდეგი მახასიათებლები:
*   წელიწადის კვირის ნომერი
*   სადღესასწაულო დღის ინდიკატორი
*   დრო, რომელიც დარჩა მომდევნო დღესასწაულამდე
*   ფურიეს გარდაქმნის მახასიათებლები (სეზონურობის აღსაქმელად)
*   ისტორიული საშუალო გაყიდვები წლის მოცემულ კვირაში (მაგ: მე-5 კვირის საშუალო წინა წლებიდან).

მნიშვნელოვანია, რომ `Store` და `Dept` სვეტები გადავაკეთეთ `int` ტიპიდან `category` ტიპზე, რათა მოდელს არ გაეთვალისწინებინა მათი რიგითობა (ანუ 1 < 2 არაფერს ნიშნავს). ასევე, არ გამოგვიყენებია `Markdown` სვეტები, რადგან ძალიან ბევრი უცნობი მნიშვნელობა პოტენციურად გააუარესებდა შედეგს.

## ეტაპი 1: მაღაზიების საშუალო გაყიდვების პროგნოზირება 

ამ ეტაპზე, დავაგრეგირეთ მონაცემები მაღაზიების მიხედვით და მივიღეთ თითოეული მაღაზიის საშუალო ყოველკვირეული გაყიდვები. დამატებით მახასიათებლად შევქმენით **მაღაზიის მთლიანი შემოსავლიანობა** (ტრენინგ მონაცემებიდან), რათა მოდელს უკეთ გაერკვია, რამდენად "დიდი" იყო თითოეული მაღაზია.

ქროს-ვალიდაციის შედეგად შერჩეული XGBoost მოდელის პარამეტრებია:

| პარამეტრი | მნიშვნელობა |
| :--- | :--- |
| `objective` | `reg:squarederror` |
| `n_estimators` | 200 |
| `learning_rate` | 0.1 |
| `max_depth` | 7 |
| `subsample` | 0.6 |
| `colsample_bytree` | 1.0 |
| `min_child_weight`| 5 |

**შედეგები:**
*   **Validation WMAE: 800.06** (MAE: 786.72)
*   **Train WMAE: 253.43** (MAE: 254.05)

*ანალიზი: მოდელი კარგად სწავლობს (დაბალი train შეცდომა) და აჩვენებს ძლიერ შედეგს ვალიდაციაზე, რაც მიუთითებს მის ეფექტურობაზე.*


## ეტაპი 2: დეპარტამენტების საშუალო გაყიდვების პროგნოზირება 

მსგავსად, დავაგრეგირეთ მონაცემები დეპარტამენტების მიხედვით და დამატებით მახასიათებლად შევქმენით დეპარტამენტის მთლიანი შემოსავლიანობა. ამ შემთხვევაში, ორიგინალური მონაცემებიდან მხოლოდ `IsHoliday` მახასიათებელი დავტოვეთ, რადგან სხვა მახასიათებლები (მაგ: ტემპერატურა, საწვავის ფასი) კონკრეტულ მაღაზიას აღწერდა და არა დეპარტამენტს ზოგადად.

ქროს-ვალიდაციით შერჩეული XGBoost მოდელის პარამეტრებია:

| პარამეტრი | მნიშვნელობა |
| :--- | :--- |
| `objective` | `reg:squarederror` |
| `n_estimators` | 300 |
| `learning_rate` | 0.1 |
| `max_depth` | 7 |
| `subsample` | 1.0 |
| `colsample_bytree` | 0.5 |
| `min_child_weight`| 1 |

**შედეგები:**
*   **Validation WMAE: 1060.54** (MAE: 1017.28)
*   **Train WMAE: 263.01** (MAE: 276.83)

*ანალიზი: შედეგი ოდნავ სუსტია `store_avg`-სთან შედარებით, რაც მოსალოდნელი იყო, რადგან ამ მოდელს ნაკლები ინფორმაციული მახასიათებელი ჰქონდა. მიუხედავად ამისა, შედეგი მაინც საკმარისად კარგია. ჩვენ ვამჯობინეთ XGBoost-ის გამოყენება, რადგან ზემოთ განხილული ნეირონული ქსელები არ გვაძლევდა საშუალებას, მიგვეღო პროგნოზები სატრენინგო მონაცემებზე, რაც აუცილებელი იყო შემდეგი ეტაპისთვის.*

## ეტაპი 3: გლობალური მოდელი

საბოლოო ეტაპზე, საწყის მონაცემებს დავუმატეთ წინა ორი ეტაპის პროგნოზები (`store_predicted_sales` და `dept_predicted_sales`) როგორც ახალი, მძლავრი მახასიათებლები. მონაცემთა ბაზის დიდი ზომის გამო, XGBoost-ის ნაცვლად გამოვიყენეთ **LightGBM**, რომელიც უფრო სწრაფია.

ქროს-ვალიდაციით მიღებული LightGBM მოდელის პარამეტრები:

| პარამეტრი | მნიშვნელობა |
| :--- | :--- |
| `objective` | `regression` |
| `n_estimators` | 1000 |
| `learning_rate` | 0.1 |
| `max_depth` | 10 |

**შედეგები:**
*   **Train WMAE: 1156.92** (MAE: 1127.47)
*   **Valid WMAE: 1953.42** (MAE: 1905.55)

**შენიშვნა:** შემდეგ მცირე ცვლილებებით, ამ მოდელის შედეგი **1821-მდე გავაუმჯობესეთ**. ორივე ვერსიის კოდი ხელმისაწვდომია რეპოზიტორში:
*   `models/walmart_group_sales.py` (v1)
*   `models/walmart_group_salesv2.py` (v2)

# საბოლოო მოდელი: ჰიბრიდული ანსამბლი

ჩვენს მიერ განხილული ორი საუკეთესო მიდგომის — ნეირონული ქსელების ანსამბლისა და GroupStat მოდელის — ანალიზმა გამოავლინა მათი უნიკალური ძლიერი და სუსტი მხარეები:

*   **ნეირონული ქსელების ანსამბლი (`nn_ensemble`)** საოცარ შედეგს აღწევს მხოლოდ დროითი მწკრივის მონაცემებზე დაყრდნობით. ის შესანიშნავად იჭერს კომპლექსურ სეზონურ პატერნებსა და ტრენდებს, თუმცა სრულად უკეთებს იგნორირებას გარე ფაქტორებს (მაგ: საწვავის ფასი, უმუშევრობის დონე).

*   **GroupStat მოდელი**, პირიქით, აგებულია გარე მახასიათებლების ეფექტურად გამოყენებაზე. მისი სიძლიერე იმაშია, რომ ის ითვალისწინებს მაღაზიისა და დეპარტამენტის კონტექსტს, თუმცა ბუნებრივად არ აღიქვამს დროით დამოკიდებულებებს.

ამ დაკვირვებამ გვიბიძგა ლოგიკური დასკვნისკენ: შევქმნათ **ჰიბრიდული მოდელი**, რომელიც გააერთიანებდა ორივე მოდელის საუკეთესო თვისებებს. იდეა იმაში მდგომარეობს, რომ შევქმნათ საბოლოო პროგნოზი, რომელიც იქნება ამ ორი, ფუნდამენტურად განსხვავებული, მოდელის პროგნოზების კომბინაცია. ამგვარად, ჩვენი საბოლოო მოდელი ერთდროულად ითვალისწინებს როგორც დროითი მწკრივის შინაგან დინამიკას, ასევე კონტექსტუალურ, გარე ფაქტორებს.

საბოლოო პროგნოზი მიიღება ამ ორი მოდელის პროგნოზების საშუალო არითმეტიკულით. წონები შევარჩიეთ, როგორც **0.5–0.5**, რაც მარტივი და ეფექტური საწყისი წერტილია. რა თქმა უნდა, ამ წონებით ექსპერიმენტირება შესაძლებელია ოპტიმალური ბალანსის საპოვნელად, თუმცა მოდელების გამოთვლითი სირთულის გამო, ეს პროცესი დიდ დროს მოითხოვს.

შედეგად მივიღეთ მოდელი, რომლის ვალიდაციის ქულა მსგავსი იყო `nn_ensemble`-ის. აღსანიშნავია, რომ ვალიდაციის შეცდომის (WMAE) კიდევ უფრო მნიშვნელოვნად შემცირება, სავარაუდოდ, რთული იქნება პროექტის ფარგლებში მკვეთრი ცვლილებების (მაგ: ფუნდამენტურად განსხვავებული მოდელირების) გარეშე, რადგან მიღწეული შედეგი უკვე კარგია. ამასთანავე, ჰიბრიდული მოდელი ბევრად უფრო მდგრადი აღმოჩნდა **გადაჭარბებული მორგების (overfitting)** მიმართ. ეს ნიშნავს, რომ ჩვენი საბოლოო მოდელი უფრო განზოგადებულია და, სავარაუდოდ, უკეთ იმუშავებს უცნობ, ახალ მონაცემებზე, რაც მას ბევრად უფრო საიმედოს ხდის.

## როგორ მუშაობს საბოლოო მოდელი (ეტაპობრივად)

რომ შევაჯამოთ, ჩვენი საბოლოო მოდელი მუშაობს შემდეგი ალგორითმით:

1.  **ნაბიჯი 1: ნეირონული ქსელების ანსამბლის პროგნოზი (შედეგი A)**
    *   ოთხი განსხვავებული ნეირონული ქსელი (PatchTST, N-BEATS, D-Linear, TFT) აკეთებს პროგნოზს მხოლოდ ისტორიული გაყიდვების მონაცემებზე დაყრდნობით.
    *   ამ ოთხი პროგნოზის შედეგი საშუალოვდება და ვიღებთ ერთ, სტაბილურ პროგნოზს.

2.  **ნაბიჯი 2: GroupStat მოდელის პროგნოზი (შედეგი B)**
    *   ცალკე მოდელებით პროგნოზირდება მაღაზიებისა და დეპარტამენტების საშუალო გაყიდვები.
    *   ეს პროგნოზები, სხვა გარე მახასიათებლებთან ერთად, გამოიყენება გლობალური LightGBM მოდელის მიერ საბოლოო პროგნოზის მისაღებად.

3.  **ნაბიჯი 3: საბოლოო ჰიბრიდული პროგნოზი**
    *   ზემოთ მიღებული ორი შედეგი (A და B) ერთიანდება მარტივი ფორმულით:
        `საბოლოო პროგნოზი = (შედეგი A * 0.5) + (შედეგი B * 0.5)`

ეს ჰიბრიდული მიდგომა უზრუნველყოფს, რომ ჩვენი მოდელი იყოს ერთდროულად ზუსტი, სტაბილური და მდგრადი.

# შედეგები

პროექტის დასასრულს, Kaggle-ის პლატფორმაზე ავტვირთეთ ჩვენი ორი საუკეთესო მოდელის პროგნოზი, რათა შეგვეფასებინა მათი რეალური წარმადობა Public და Private ლიდერბორდებზე.

| მოდელი | Public Score (WMAE) | Private Score (WMAE) |
| :--- | :--- | :--- |
| ნეირონული ქსელების ანსამბლი | 2729.61 | 2635.51 |
| **საბოლოო ჰიბრიდული მოდელი** | **2691.28** | **2587.67** |

როგორც შედეგებიდან ნათლად ჩანს, ჩვენი **საბოლოო, ჰიბრიდული მოდელი** ორივე ლიდერბორდზე უკეთეს შედეგს აჩვენებს. ეს ადასტურებს ჩვენს მთავარ ჰიპოთეზას: GroupStat მოდელის დამატებამ, რომელიც ითვალისწინებს გარე მახასიათებლებს, მნიშვნელოვნად გააძლიერა მხოლოდ დროით მწკრივებზე დაფუძნებული ნეირონული ქსელების ანსამბლი.

საბოლოო ჯამში, ორი ფუნდამენტურად განსხვავებული მიდგომის გაერთიანებამ მოგვცა საშუალება, მიგვეღო საუკეთესო შედეგი, რომელიც ერთდროულად ეფუძნება როგორც დროითი მწკრივის შიდა დინამიკას, ისე გარე ფაქტორებს.
